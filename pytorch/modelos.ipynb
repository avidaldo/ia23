{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db412e85",
   "metadata": {},
   "source": [
    "# Construyendo modelos con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c2e0e",
   "metadata": {},
   "source": [
    "## Modelos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn.Module` y `torch.nn.Parameter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`torch.nn.Module` es la clase base para cualquier red neuronal en PyTorch. Cualquier clase que herede de `torch.nn.Module` debe implementar el método `forward`. El método `forward` es el que define cómo se calcula la salida de la red neuronal.\n",
    "\n",
    "Cualquier objeto de tipo `torch.nn.Module` registra todos los parámetros de la red neuronal (los pesos y los sesgos). Estos parámetros son objetos de tipo `torch.nn.Parameter`, que es una subclase de `torch.Tensor`. Los parámetros se pueden acceder a través del método `parameters()` de la clase `Module`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo de cómo definir una red neuronal muy básica en PyTorch. Esta red consta de dos capas lineales y una función de activación ReLU entre ellas. En ella podemos ver la estructura básica de una red neuronal en PyTorch, con un método `__init__()` que define las capas y otros componentes de la red, y un método `forward()` donde se realiza la computación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0165, -0.0320, -0.0229,  ..., -0.0139, -0.0368,  0.0666],\n",
      "        [-0.0647,  0.0893, -0.0493,  ...,  0.0397, -0.0546, -0.0781],\n",
      "        [-0.0740, -0.0500, -0.0597,  ...,  0.0890, -0.0705, -0.0604],\n",
      "        ...,\n",
      "        [-0.0829, -0.0146,  0.0345,  ..., -0.0057,  0.0829,  0.0737],\n",
      "        [-0.0223, -0.0434, -0.0292,  ..., -0.0340,  0.0451,  0.0523],\n",
      "        [-0.0747, -0.0579,  0.0683,  ...,  0.0150, -0.0853,  0.0460]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0587, -0.0109,  0.0101, -0.0069,  0.0317,  0.0026,  0.0690, -0.0775,\n",
      "        -0.0985,  0.0605, -0.0362,  0.0442,  0.0301, -0.0218, -0.0363,  0.0097,\n",
      "         0.0359,  0.0279, -0.0797, -0.0368, -0.0204, -0.0146, -0.0700,  0.0896,\n",
      "         0.0429, -0.0466,  0.0119, -0.0401,  0.0614,  0.0954,  0.0135,  0.0598,\n",
      "        -0.0901, -0.0858,  0.0289,  0.0391,  0.0965, -0.0035,  0.0616,  0.0662,\n",
      "        -0.0113,  0.0824,  0.0218, -0.0448, -0.0395, -0.0603, -0.0374,  0.0433,\n",
      "         0.0313,  0.0902, -0.0963, -0.0342,  0.0986,  0.0246, -0.0249, -0.0651,\n",
      "         0.0778, -0.0705, -0.0454,  0.0535,  0.0802, -0.0550, -0.0826, -0.0011,\n",
      "        -0.0772, -0.0826, -0.0865,  0.0194,  0.0561, -0.0706,  0.0364,  0.0487,\n",
      "        -0.0305,  0.0986, -0.0772, -0.0327,  0.0845, -0.0090, -0.0836,  0.0278,\n",
      "         0.0520, -0.0221,  0.0371,  0.0015,  0.0162,  0.0931, -0.0587,  0.0759,\n",
      "         0.0545, -0.0988, -0.0414, -0.0564, -0.0349, -0.0190, -0.0561,  0.0395,\n",
      "        -0.0936, -0.0932,  0.0478,  0.0496,  0.0685,  0.0468, -0.0572, -0.0800,\n",
      "        -0.0203, -0.0652,  0.0460, -0.0948,  0.0881,  0.0456,  0.0104, -0.0890,\n",
      "         0.0765,  0.0384, -0.0707,  0.0017,  0.0846,  0.0050, -0.0111,  0.0554,\n",
      "        -0.0215, -0.0934,  0.0366,  0.0251, -0.0594, -0.0612,  0.0555,  0.0741,\n",
      "         0.0364, -0.0553,  0.0802,  0.0208, -0.0433,  0.0877, -0.0151,  0.0035,\n",
      "         0.0476, -0.0641, -0.0110, -0.0706,  0.0154,  0.0523,  0.0320, -0.0394,\n",
      "         0.0451, -0.0391, -0.0388, -0.0867, -0.0578,  0.0176,  0.0352, -0.0849,\n",
      "        -0.0267,  0.0194, -0.0600, -0.0401, -0.0252,  0.0970,  0.0120,  0.0277,\n",
      "        -0.0493,  0.0200, -0.0353,  0.0911,  0.0677, -0.0935, -0.0642,  0.0386,\n",
      "        -0.0899, -0.0097,  0.0922,  0.0592,  0.0888,  0.0107,  0.0820, -0.0833,\n",
      "        -0.0807, -0.0879,  0.0138,  0.0347,  0.0153, -0.0914, -0.0912,  0.0708,\n",
      "        -0.0401,  0.0598, -0.0303,  0.0937, -0.0943, -0.0149,  0.0298,  0.0220,\n",
      "         0.0247, -0.0959,  0.0681,  0.0505, -0.0848, -0.0641,  0.0820, -0.0942],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040,  0.0597,  0.0378,  ...,  0.0153, -0.0004,  0.0695],\n",
      "        [ 0.0010, -0.0103, -0.0546,  ..., -0.0295,  0.0518, -0.0104],\n",
      "        [ 0.0472,  0.0428, -0.0081,  ...,  0.0586,  0.0193, -0.0179],\n",
      "        ...,\n",
      "        [-0.0034, -0.0559,  0.0353,  ...,  0.0464, -0.0445,  0.0047],\n",
      "        [ 0.0684,  0.0276, -0.0268,  ...,  0.0538,  0.0426,  0.0547],\n",
      "        [-0.0414, -0.0067,  0.0659,  ...,  0.0375, -0.0407, -0.0072]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0330,  0.0631,  0.0446,  0.0035, -0.0126,  0.0349, -0.0547, -0.0270,\n",
      "         0.0034,  0.0137], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0040,  0.0597,  0.0378,  ...,  0.0153, -0.0004,  0.0695],\n",
      "        [ 0.0010, -0.0103, -0.0546,  ..., -0.0295,  0.0518, -0.0104],\n",
      "        [ 0.0472,  0.0428, -0.0081,  ...,  0.0586,  0.0193, -0.0179],\n",
      "        ...,\n",
      "        [-0.0034, -0.0559,  0.0353,  ...,  0.0464, -0.0445,  0.0047],\n",
      "        [ 0.0684,  0.0276, -0.0268,  ...,  0.0538,  0.0426,  0.0547],\n",
      "        [-0.0414, -0.0067,  0.0659,  ...,  0.0375, -0.0407, -0.0072]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0330,  0.0631,  0.0446,  0.0035, -0.0126,  0.0349, -0.0547, -0.0270,\n",
      "         0.0034,  0.0137], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self): # Definimos las capas como atributos \n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(100, 200) # Capa de entrada\n",
    "        self.activation = torch.nn.ReLU() # Función de activación\n",
    "        self.linear2 = torch.nn.Linear(200, 10) # Capa de salida\n",
    "        self.softmax = torch.nn.Softmax() # Función de salida\n",
    "    \n",
    "    def forward(self, x): # Definimos el flujo de datos\n",
    "        x = self.linear1(x) # Capa de entrada\n",
    "        x = self.activation(x) # Función de activación\n",
    "        x = self.linear2(x) # Capa de salida\n",
    "        x = self.softmax(x) # Función de salida\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estilo funcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html) permite llamar a algunos de los elementos (típicamente funciones de activación) directamente como funciones en lugar de cómo atributos de un objeto. Por ejemplo, el modelo anterior es equivalente al siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(100, 200) # Capa de entrada\n",
    "        self.activation = torch.nn.ReLU() # Función de activación\n",
    "        self.linear2 = torch.nn.Linear(200, 10) # Capa de salida\n",
    "        self.softmax = torch.nn.Softmax() # Función de salida\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) \n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.linear2 = torch.nn.Linear(200, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    # o también es equivalente a:\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.softmax(self.linear2(x))\n",
    "        return x\n",
    "    \n",
    "    # o aún más compacto:\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear2(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando `Secuential`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` es una clase que permite definir una red neuronal secuencialmente. Es decir, se pueden definir las capas de la red neuronal en el orden en el que se van a aplicar. A continuación se muestra cómo se puede definir el modelo anterior usando `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.model = nn.Sequential( # Definimos las capas en orden como un único atributo \n",
    "        nn.Linear(100, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 10),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.model(x) # Se llama a toda la secuencia, su orden ya está definido internamente\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tipo más básico de capa de red neuronal es una capa *lineal* o *totalmente conectada*. Esta es una capa en la que cada entrada influye en cada salida de la capa en un grado especificado por los pesos de la capa. Si un modelo tiene *m* entradas y *n* salidas, los pesos serán una matriz *m* x *n*.\n",
    "\n",
    "Se llama *lineal* porque la salida de la capa es una combinación lineal de las entradas $y=Wx+b$, donde $W$ es la matriz de pesos, $x$ es el vector de entradas y $b$ es el vector de sesgos.\n",
    "\n",
    "Si tenemos 3 entradas $x_1$, $x_2$ y $x_3$ y 2 salidas $y_1$ y $y_2$, la salida de la capa será:\n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros de la capa (pesos y sesgos):\n",
      "Parameter containing:\n",
      "tensor([[-0.2069,  0.2911, -0.0554],\n",
      "        [-0.3530, -0.0932,  0.4839]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5717, -0.4867], requires_grad=True)\n",
      "\n",
      "\n",
      "Pesos: Parameter containing:\n",
      "tensor([[-0.2069,  0.2911, -0.0554],\n",
      "        [-0.3530, -0.0932,  0.4839]], requires_grad=True)\n",
      "\n",
      "\n",
      "Sesgos: Parameter containing:\n",
      "tensor([-0.5717, -0.4867], requires_grad=True)\n",
      "\n",
      "\n",
      "Input: tensor([[0.4343, 0.9277, 0.8893]])\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[-0.4408, -0.2962]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2) # 3 entradas, 2 salidas\n",
    "\n",
    "print('Parámetros de la capa (pesos y sesgos):')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "# Lo mismo accediendo directamente a cada atributo:\n",
    "print('\\n\\nPesos:', lin.weight)\n",
    "print('\\n\\nSesgos:', lin.bias)\n",
    "\n",
    "x = torch.rand(1, 3) # Tensor de entrada de 1x3\n",
    "print('\\n\\nInput:', x)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que `lin.weight` contiene la matriz de pesos y que `lin.bias` contiene el vector de sesgos, siendo ambos de tipo `Parameter`.\n",
    "\n",
    "`Parameter` es una subclase de `Tensor` que se utiliza para indicar que un tensor es un parámetro de una red neuronal y, por lo tanto, debe registrar los gradientes por el módulo de autograd de PyTorch. Esto es importante para que PyTorch pueda calcular los gradientes de los parámetros durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
