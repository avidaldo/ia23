{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La librería PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librería creada en 2016 por FAIR (Facebook AI Research) y liberada en 2018, basada en Torch, una librería de aprendizaje profundo en C++, pero centrada en Python. Desde su lanzamiento, PyTorch ha sido adoptado por una gran comunidad de investigadores y desarrolladores, llegando a competir (o incluso superar) a TensorFlow en muchos aspectos.\n",
    "\n",
    "PyTorch destaca por su flexibilidad y lo pythonico (diseñado suguiendo los principios y [estilo de Python](https://peps.python.org/pep-0008/)) que es, lo que la hace más fácil de usar y de depurar que TensorFlow.\n",
    "\n",
    "\n",
    "[![vs](img/pytorch_vs_tf.png)](https://github.com/diegoandradecanosa/CFR24/blob/main/slides/02_pytorchParte1P.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar pytorch, debemos seguir las [instrucciones de la página web](https://pytorch.org/get-started/locally/), donde nos permite configurar la versión de PyTorch (trabajaremos con la versión estable), el SO, el lenguaje (Python), y la plataforma hardware que se usará para computar.\n",
    "\n",
    "Quien tenga una tarjeta gráfica NVIDIA, puede instalar la versión de PyTorch que incluye soporte para CUDA, lo que permite acelerar los cálculos en la GPU. \n",
    "\n",
    "PyTorch no incluye soporte nativo para tarjetas gráficas AMD, pero se puede instalar en linux la plataforma open-source [ROCm](https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/) para utilizarlas.\n",
    "\n",
    "Por ejemplo, para crear un entorno de conda con PyTorch en un sistema Linux sin GPU, se pueden usar estos comandos:\n",
    "\n",
    "```bash\n",
    "conda create -n env_torch\n",
    "conda activate env_torch\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "\n",
    "Podemos comprobar que ha sido correctamente instalada ejecutando la siguiente celda para importarla y ver la versión instalada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Pytorch: 2.3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Versión de Pytorch:\",torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tensores son la estructura de datos básica en PyTorch, y son similares a los arrays de NumPy. Son arrays multidimensionales que pueden representar datos, parámetros de modelos, gradientes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.int64\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # Tensor a partir de una lista\n",
    "print(t1) # Imprime el tensor\n",
    "# no especifica el tipo porque infiere el por defecto para int (int64)\n",
    "print(t1.dtype) # Tipo de los elementos (inferido)\n",
    "print(t1.shape) # Forma del tensor (dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Tensor de 1D a partir de un array de numpy\n",
    "print(torch.from_numpy(np.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.9086, 0.4982],\n",
      "        [0.2959, 0.4498]])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch puede crear tensores con valores específicos del mismo modo que NumPy\n",
    "print(torch.zeros(2, 2))\n",
    "print(torch.ones(2, 2))\n",
    "print(torch.rand(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3], dtype=torch.float32) # Tensor especificando tipo float32 en lugar de inferirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  6],\n",
      "        [ 8, 10, 12]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[ 1,  4,  9],\n",
      "        [16, 25, 36]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "print(t1 + t1) # Suma elemento a elemento\n",
    "print(t1 - t1) # Resta elemento a elemento\n",
    "print(t1 * t1) # Multiplicación elemento a elemento\n",
    "print(t1 / t1) # División elemento a elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n"
     ]
    }
   ],
   "source": [
    "print(t1.T) # Transpuesta de un tensor\n",
    "\n",
    "print(torch.matmul(t1, t2)) # Producto matricial\n",
    "print(t1 @ t2) # Producto matricial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor(5)\n",
      "tensor([[4, 5, 6]])\n",
      "tensor([4, 5, 6])\n",
      "tensor([2, 5])\n",
      "tensor([5, 6])\n",
      "tensor([[5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(t1[0]) # Primer elemento\n",
    "print(t1[1, 1]) # Elemento en la fila 1, columna 1\n",
    "print(t1[1:]) # Desde el segundo elemento en adelante\n",
    "print(t1[1, :]) # Fila 1\n",
    "print(t1[:, 1]) # Columna 1\n",
    "print(t1[1, 1:]) # Fila 1, desde el segundo elemento en adelante\n",
    "print(t1[1:, 1:]) # Submatriz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de GPGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las últimas dos décadas, se ha venido utilizando el paralelismo de las GPUs para acelerar los cálculos en el aprendizaje profundo, en lo que se conoce como ***GPGPU*** (General-Purpose computing on Graphics Processing Units).\n",
    "\n",
    "- [Xataka.com - Las GPU como pasado, presente y futuro de la computación](https://www.xataka.com/componentes/las-gpu-como-pasado-presente-y-futuro-de-la-computacion)\n",
    "- [Dot CSV: ¿Por qué las GPUs son buenas para la IA? (con resumen acelerado de arquitectura de ordenadores)](https://www.youtube.com/watch?v=C_wSHKG8_fg)\n",
    "\n",
    "Numpy no permite el uso de GPUs (aunque existen otras librerías específicas como [CuPy](https://cupy.dev/) para procesar arrays usando [CUDA](https://es.wikipedia.org/wiki/CUDA)). Sin embargo, las librerías de *deep learning*, como **PyTorch**, sí permiten el uso de GPUs para acelerar los cálculos. \n",
    "\n",
    "PyTorch permite nativamente usar GPUs de NVIDIA a través de la [CUDA](https://es.wikipedia.org/wiki/CUDA), así como las de Apple a través de [Metal](https://en.wikipedia.org/wiki/Metal_(API)). Las GPU de AMD se pueden utilizar a través de [ROCm](https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/).\n",
    "\n",
    "Los tensores permiten definir dónde van a ser almacenados. De este modo, se puede indicar explícitamente que se desea almacenar un tensor en la GPU, lo que permite acelerar los cálculos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1650 is available.\n"
     ]
    }
   ],
   "source": [
    "aval_device = (\n",
    "    \"cuda\" if torch.cuda.is_available() # CUDA: Compute Unified Device Architecture (NVIDIA)\n",
    "    else \"mps\" if torch.backends.mps.is_available() # MPS: Metal Performance Shaders (Apple)\n",
    "    #else \"hip\" if torch.hip.is_available() # HIP: Heterogeneous-compute Interface for Portability (AMD - ROCm)\n",
    "    else \"cpu\" \n",
    ")\n",
    "\n",
    "if aval_device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "elif aval_device == \"mps\":\n",
    "    print(\"MPS is available.\")\n",
    "elif aval_device == \"hip\":\n",
    "    print(\"HIP is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, si se dispone de una GPU, se puede ver cómo PyTorch no permite realizar operaciones entre tensores que no están en el mismo dispositivo y cómo moverlos con el método `.to()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensor in method wrapper_CUDA__dot)\n",
      "tensor(14., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1., 2.], [3., 4.]], device=\"cpu\") # Tensor en la CPU\n",
    "print(t1.device)\n",
    "\n",
    "if torch.cuda.is_available(): # Si hay GPU disponible\n",
    "    \n",
    "    t2 = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=\"cuda\") # Tensor en la GPU\n",
    "    \n",
    "    try:\n",
    "        print(torch.matmul(t1, t2)) # Error: no se pueden multiplicar tensores en diferentes dispositivos.\n",
    "    except RuntimeError as e: \n",
    "        print(\"RuntimeError:\", e)\n",
    "        \n",
    "    t1 = t1.to(\"cuda\") # Movemos el tensor a la GPU\n",
    "    print(torch.matmul(t1, t2)) # Multiplicando en la GPU\n",
    "    \n",
    "else:\n",
    "    print(\"No tienes GPU para poder testear el código anterior.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
